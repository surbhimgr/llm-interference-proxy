// Code generated by protoc-gen-go. DO NOT EDIT.
// source: proto/proxy.proto

package proto

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
)

const (
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// InferenceRequest represents a client request to an LLM provider.
type InferenceRequest struct {
	state         protoimpl.MessageState
	sizeCache     protoimpl.SizeCache
	unknownFields protoimpl.UnknownFields

	Model       string  `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	Prompt      string  `protobuf:"bytes,2,opt,name=prompt,proto3" json:"prompt,omitempty"`
	Temperature float32 `protobuf:"fixed32,3,opt,name=temperature,proto3" json:"temperature,omitempty"`
	MaxTokens   int32   `protobuf:"varint,4,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`
}

func (x *InferenceRequest) Reset()         { *x = InferenceRequest{} }
func (x *InferenceRequest) String() string { return protoimpl.X.MessageStringOf(x) }
func (x *InferenceRequest) ProtoMessage()  {}

func (x *InferenceRequest) ProtoReflect() protoreflect.Message {
	return nil // simplified stub
}

func (x *InferenceRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *InferenceRequest) GetPrompt() string {
	if x != nil {
		return x.Prompt
	}
	return ""
}

func (x *InferenceRequest) GetTemperature() float32 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *InferenceRequest) GetMaxTokens() int32 {
	if x != nil {
		return x.MaxTokens
	}
	return 0
}

// InferenceResponse represents the full response from an LLM provider.
type InferenceResponse struct {
	state         protoimpl.MessageState
	sizeCache     protoimpl.SizeCache
	unknownFields protoimpl.UnknownFields

	Text         string  `protobuf:"bytes,1,opt,name=text,proto3" json:"text,omitempty"`
	PromptTokens int32   `protobuf:"varint,2,opt,name=prompt_tokens,json=promptTokens,proto3" json:"prompt_tokens,omitempty"`
	OutputTokens int32   `protobuf:"varint,3,opt,name=output_tokens,json=outputTokens,proto3" json:"output_tokens,omitempty"`
	CacheHit     bool    `protobuf:"varint,4,opt,name=cache_hit,json=cacheHit,proto3" json:"cache_hit,omitempty"`
	LatencyMs    float64 `protobuf:"fixed64,5,opt,name=latency_ms,json=latencyMs,proto3" json:"latency_ms,omitempty"`
}

func (x *InferenceResponse) Reset()         { *x = InferenceResponse{} }
func (x *InferenceResponse) String() string { return protoimpl.X.MessageStringOf(x) }
func (x *InferenceResponse) ProtoMessage()  {}

func (x *InferenceResponse) ProtoReflect() protoreflect.Message {
	return nil // simplified stub
}

func (x *InferenceResponse) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *InferenceResponse) GetPromptTokens() int32 {
	if x != nil {
		return x.PromptTokens
	}
	return 0
}

func (x *InferenceResponse) GetOutputTokens() int32 {
	if x != nil {
		return x.OutputTokens
	}
	return 0
}

func (x *InferenceResponse) GetCacheHit() bool {
	if x != nil {
		return x.CacheHit
	}
	return false
}

func (x *InferenceResponse) GetLatencyMs() float64 {
	if x != nil {
		return x.LatencyMs
	}
	return 0
}

// StreamChunk represents a single chunk in a streaming response.
type StreamChunk struct {
	state         protoimpl.MessageState
	sizeCache     protoimpl.SizeCache
	unknownFields protoimpl.UnknownFields

	Text         string `protobuf:"bytes,1,opt,name=text,proto3" json:"text,omitempty"`
	Done         bool   `protobuf:"varint,2,opt,name=done,proto3" json:"done,omitempty"`
	PromptTokens int32  `protobuf:"varint,3,opt,name=prompt_tokens,json=promptTokens,proto3" json:"prompt_tokens,omitempty"`
	OutputTokens int32  `protobuf:"varint,4,opt,name=output_tokens,json=outputTokens,proto3" json:"output_tokens,omitempty"`
}

func (x *StreamChunk) Reset()         { *x = StreamChunk{} }
func (x *StreamChunk) String() string { return protoimpl.X.MessageStringOf(x) }
func (x *StreamChunk) ProtoMessage()  {}

func (x *StreamChunk) ProtoReflect() protoreflect.Message {
	return nil // simplified stub
}

func (x *StreamChunk) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *StreamChunk) GetDone() bool {
	if x != nil {
		return x.Done
	}
	return false
}

func (x *StreamChunk) GetPromptTokens() int32 {
	if x != nil {
		return x.PromptTokens
	}
	return 0
}

func (x *StreamChunk) GetOutputTokens() int32 {
	if x != nil {
		return x.OutputTokens
	}
	return 0
}

// File descriptor stubs â€” in production, these would be generated by protoc.
var _ protoreflect.Message
var _ reflect.Type
var _ sync.Mutex
